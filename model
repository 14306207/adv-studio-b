from google.colab import drive
drive.mount('/content/drive')
import os
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import torch
import tensorflow as tf
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, BatchNormalization, Add, Conv2DTranspose, SpatialDropout2D, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import numpy as np
import glob

path =  '/content/drive/My Drive/covid'
# Check if GPU is available
print("CUDA available:", torch.cuda.is_available())

# Print GPU device name
if torch.cuda.is_available():
    print("GPU Name:", torch.cuda.get_device_name(0))
# Check if GPU is available
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

img_dir = os.path.join(path, 'img')
mask_dir = os.path.join(path, 'labels')

img_list = sorted(glob.glob(os.path.join(img_dir, '*'))) 
mask_list = sorted(glob.glob(os.path.join(mask_dir, '*')))

total_img = len(img_list)
total_mask = len(mask_list)
print(f"There are {total_img} images in the dataset")
print(f"There are {total_mask} masks in the dataset")

index = 0
img_path = os.path.join(img_dir, img_list[index])
mask_path = os.path.join(mask_dir, mask_list[index])

img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED) 
mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

if img.ndim == 3 and img.shape[2] == 3:  
    plt.imshow(img) 
else:
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.title("Image")

plt.subplot(1, 2, 2)
plt.title("Mask")
plt.imshow(mask, cmap='gray')

plt.show()

img_dir = os.path.join(path, 'img')
mask_dir = os.path.join(path, 'labels')

img_list = sorted(glob.glob(os.path.join(img_dir, '*')))
mask_list = sorted(glob.glob(os.path.join(mask_dir, '*')))


total_images = len(img_list)  
total_masks = len(mask_list)  

first_img_path = img_list[0]  
first_mask_path = mask_list[0] 

first_img = cv2.imread(first_img_path, cv2.IMREAD_GRAYSCALE)
first_mask = cv2.imread(first_mask_path, cv2.IMREAD_GRAYSCALE)

img_shape = first_img.shape
mask_shape = first_mask.shape

images = np.zeros((total_images, *img_shape), dtype=first_img.dtype)
masks = np.zeros((total_masks, *mask_shape), dtype=first_mask.dtype)

for i, (img_file, mask_file) in enumerate(zip(img_list, mask_list)):  
    img_path = os.path.join(img_dir, img_file)  
    mask_path = os.path.join(mask_dir, mask_file)
    
    images[i] = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    masks[i] = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    print(f'Image {img_file} shape: {images[i].shape}')
    print(f'Mask {mask_file} shape: {masks[i].shape}')

print(images.shape)
print(masks.shape)
mean = np.mean(images) 
std = np.std(images) 

X = images.astype(np.float32)
X -= mean
X /= std 

Y = masks.astype(np.float32) 
Y /= 255.0 

print("X shape:", X.shape)
print("y shape:", Y.shape)

print("X unique values:", np.unique(X))
print("Y unique values:", np.unique(Y))

datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    fill_mode='nearest'
)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=21)

num_folds = 3
kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)  
histories = []

for fold, (train_index, val_index) in enumerate(kfold.split(X)):
    print(f"Training on fold {fold + 1}...")
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = Y[train_index], Y[val_index]

def feature_pyramid_network(C3, C4, C5):
    """Creates a Feature Pyramid Network (FPN) from given feature maps."""
    # Lateral raise connections
    P5 = Conv2D(256, (1, 1), name='fpn_c5p5')(C5)
    P4 = Add(name="fpn_p4add")([
        UpSampling2D(size=(2, 2), name="fpn_p5upsampled")(P5),
        Conv2D(256, (1, 1), name='fpn_c4p4')(C4)])
    P3 = Add(name="fpn_p3add")([
        UpSampling2D(size=(2, 2), name="fpn_p4upsampled")(P4),
        Conv2D(256, (1, 1), name='fpn_c3p3')(C3)])

    # Smoothing
    P3 = Conv2D(256, (3, 3), padding="SAME", name="fpn_p3")(P3)
    P4 = Conv2D(256, (3, 3), padding="SAME", name="fpn_p4")(P4)
    P5 = Conv2D(256, (3, 3), padding="SAME", name="fpn_p5")(P5)

    return P3, P4, P5

def unet_model(input_size=(256, 256, 1)):
    inputs = Input(input_size)

    # Contractions
    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    c1 = BatchNormalization()(c1)
    c1 = Dropout(0.1)(c1)
    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(c1)
    c1 = BatchNormalization()(c1)
    p1 = MaxPooling2D((2, 2))(c1)

    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)
    c2 = BatchNormalization()(c2)
    c2 = Dropout(0.1)(c2)
    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(c2)
    c2 = BatchNormalization()(c2)
    p2 = MaxPooling2D((2, 2))(c2)

    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)
    c3 = BatchNormalization()(c3)
    c3 = Dropout(0.2)(c3)
    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(c3)
    c3 = BatchNormalization()(c3)
    p3 = MaxPooling2D((2, 2))(c3)

    c4 = Conv2D(256, (3, 3), activation='relu', padding='same')(p3)
    c4 = BatchNormalization()(c4)
    c4 = Dropout(0.2)(c4)
    c4 = Conv2D(256, (3, 3), activation='relu', padding='same')(c4)
    c4 = BatchNormalization()(c4)
    p4 = MaxPooling2D((2, 2))(c4)

    c5 = Conv2D(512, (3, 3), activation='relu', padding='same')(p4)
    c5 = BatchNormalization()(c5)
    c5 = Dropout(0.3)(c5)
    c5 = Conv2D(512, (3, 3), activation='relu', padding='same')(c5)
    c5 = BatchNormalization()(c5)

    # Feature Pyramid Network
    P3, P4, P5 = feature_pyramid_network(c3, c4, c5)

    # Domain expansion
    u6 = UpSampling2D((2, 2))(c5)
    u6 = concatenate([u6, P4])  # Using P4 from FPN
    c6 = Conv2D(256, (3, 3), activation='relu', padding='same')(u6)
    c6 = BatchNormalization()(c6)
    c6 = Dropout(0.2)(c6)
    c6 = Conv2D(256, (3, 3), activation='relu', padding='same')(c6)
    c6 = BatchNormalization()(c6)

    u7 = UpSampling2D((2, 2))(c6)
    u7 = concatenate([u7, P3])  # Using P3 from FPN
    c7 = Conv2D(128, (3, 3), activation='relu', padding='same')(u7)
    c7 = BatchNormalization()(c7)
    c7 = Dropout(0.2)(c7)
    c7 = Conv2D(128, (3, 3), activation='relu', padding='same')(c7)
    c7 = BatchNormalization()(c7)

    u8 = UpSampling2D((2, 2))(c7)
    u8 = concatenate([u8, c2])
    c8 = Conv2D(64, (3, 3), activation='relu', padding='same')(u8)
    c8 = BatchNormalization()(c8)
    c8 = Dropout(0.1)(c8)
    c8 = Conv2D(64, (3, 3), activation='relu', padding='same')(c8)
    c8 = BatchNormalization()(c8)

    u9 = UpSampling2D((2, 2))(c8)
    u9 = concatenate([u9, c1])
    c9 = Conv2D(32, (3, 3), activation='relu', padding='same')(u9)
    c9 = BatchNormalization()(c9)
    c9 = Dropout(0.1)(c9)
    c9 = Conv2D(32, (3, 3), activation='relu', padding='same')(c9)
    c9 = BatchNormalization()(c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)

    model = Model(inputs=[inputs], outputs=[outputs])
    return model


def dice_coefficient(y_true, y_pred, smooth=1.0):
    y_true_f = tf.reshape(y_true, [-1])  
    y_pred_f = tf.reshape(y_pred, [-1])   
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)
    return score

def iou_metric(y_true, y_pred, smooth=1.0):
    y_true_f = tf.reshape(y_true, [-1]) 
    y_pred_f = tf.reshape(y_pred, [-1]) 
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection
    iou = (intersection + smooth) / (union + smooth)
    return iou

input_size = (128, 128, 1)
model = unet_model(input_size)
learning_rate = 0.008

optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
loss_fn = tf.keras.losses.BinaryCrossentropy()

callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=5),
]

train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.batch(10).prefetch(tf.data.AUTOTUNE)

val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))
val_dataset = val_dataset.batch(10).prefetch(tf.data.AUTOTUNE)

model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', dice_coefficient, iou_metric])

early_stopping = EarlyStopping(patience=5, restore_best_weights=True)
history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, callbacks=callbacks)

batch_size = 32  
predictions = model.predict(X_test, batch_size=batch_size)

num_predictions = X_test.shape[0]
print("Number of predictions made:", num_predictions)

index = 8  

raw_image = X_test[index]
original_mask = y_test[index]
predicted_mask = predictions[index]

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.imshow(np.squeeze(original_mask), cmap='gray')  
plt.title('Original Mask')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(np.squeeze(predicted_mask), cmap='gray')  
plt.title('Predicted Mask')
plt.axis('off')

plt.show()
predicted_mask = (predicted_mask > 0.5).astype(np.float32)

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

plt.show()

training_accuracy = history.history['accuracy'][-1]  
validation_accuracy = history.history['val_accuracy'][-1]  
dice_score = history.history['dice_coefficient'][-1]
iou = history.history['iou_metric'][-1]

print(f"Training Accuracy: {training_accuracy:.4f}")
print(f"Validation Accuracy: {validation_accuracy:.4f}")
print(f"Dice Coefficient: {dice_score:.4f}")
print(f"iou: {iou:.4f}")
